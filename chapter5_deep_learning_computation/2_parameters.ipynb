{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.4144],\n        [0.3488]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(8,1))\n",
    "X = torch.rand(size=(2,4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第二层网络的状态（包括权重和偏差）：OrderedDict([('weight', tensor([[-0.0158, -0.3498,  0.2600,  0.2497, -0.0741,  0.1854,  0.3060, -0.1925]])), ('bias', tensor([0.3488]))])\n",
      "偏差的类型：<class 'torch.nn.parameter.Parameter'>\n",
      "查看偏差（包括值和梯度）：Parameter containing:\n",
      "tensor([0.3488], requires_grad=True)\n",
      "查看偏差的值（data）：tensor([0.3488])\n",
      "查看偏差的梯度（grad）：None\n",
      "(由于没有做梯度下降所有grad=None)\n"
     ]
    }
   ],
   "source": [
    "# net[2]拿到的是nn.Linear(8,1)，state_dict是权重和偏差\n",
    "print(f'第二层网络的状态（包括权重和偏差）：{net[2].state_dict()}')\n",
    "# bias是一个Parameter，也就是可优化的参数\n",
    "print(f'偏差的类型：{type(net[2].bias)}')\n",
    "print(f'查看偏差（包括值和梯度）：{net[2].bias}')\n",
    "print(f'查看偏差的值（data）：{net[2].bias.data}')\n",
    "print(f'查看偏差的梯度（grad）：{net[2].bias.grad}\\n(由于没有做梯度下降所以grad=None)')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name,param.shape) for name,param in net[0].named_parameters()])\n",
    "print(*[(name,param.shape) for name,param in net.named_parameters()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[-0.2472,  0.0148, -0.1239,  0.4094],\n",
      "        [ 0.2604,  0.1003, -0.4057,  0.1262],\n",
      "        [-0.1069,  0.1446,  0.4027,  0.2727],\n",
      "        [-0.2044,  0.4485,  0.0592, -0.4451],\n",
      "        [-0.0709,  0.0416, -0.0018, -0.3404],\n",
      "        [-0.3940, -0.0842,  0.0229, -0.2976],\n",
      "        [ 0.0410, -0.4246, -0.1798, -0.4027],\n",
      "        [-0.4664, -0.2219, -0.1327, -0.1411]])), ('0.bias', tensor([-0.1172, -0.1561, -0.3075, -0.3918,  0.0839, -0.3010,  0.1176, -0.3494])), ('2.weight', tensor([[-0.0158, -0.3498,  0.2600,  0.2497, -0.0741,  0.1854,  0.3060, -0.1925]])), ('2.bias', tensor([0.3488]))])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([0.3488])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net.state_dict())\n",
    "net.state_dict()['2.bias'].data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.1193],\n        [0.1193]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(8,4),\n",
    "                         nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 传入4个block1\n",
    "        # f'block {i}':传一个字符串的名字\n",
    "        net.add_module(f'block {i}',block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(),nn.Linear(4,1))\n",
    "rgnet(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 0.0126, -0.0043, -0.0059, -0.0189]), tensor(0.))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    '''\n",
    "    用正态分布初始化权重，偏差初始化为0\n",
    "    :param m: 层\n",
    "    :return:\n",
    "    '''\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# 对于net里面的所有module调用init_normal\n",
    "# 如果是嵌套就是嵌套遍历\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0664,  0.3901,  0.4517, -0.6281])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight,42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[-0.0000, -0.0000, 0.0000, 0.0000],\n        [-0.0000, -0.0000, 9.9754, 8.4712]], grad_fn=<SliceBackward0>)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\n",
    "            \"Init\",\n",
    "            *[(name,param.shape) for name,param in m.named_parameters()][0]\n",
    "        )\n",
    "        nn.init.uniform_(m.weight,-10,10)\n",
    "        # 将绝对值小于5的权重置零\n",
    "        m.weight.data *= m.weight.data.abs()>=5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([42.,  1.,  1.,  1.])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接取出替换\n",
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0,0] = 42\n",
    "net[0].weight.data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 参数绑定\n",
    "shared = nn.Linear(8,8)\n",
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),\n",
    "                    shared,nn.ReLU(),\n",
    "                    shared,nn.ReLU(),\n",
    "                    nn.Linear(8,1))\n",
    "net(X)\n",
    "# 两个share是一样的\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0,0] = 100\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
